{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Install Tensorflow Object Detection API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the step by step official [installation guide](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Find a dataset to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, I'm using a handgun dataset (over 3000 pictures) you can [download here](https://sci2s.ugr.es/weapons-detection). For a better prediction, you'll have to gather a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Label the desired objects in each picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using [RectLabel](https://rectlabel.com/) on MacOS but there are a lot of free alternatives such as [LabelImg](https://github.com/tzutalin/labelImg). Both they do the job pretty well. The choice is up to you.\n",
    "\n",
    "Tips: Before labeling pictures, chunk your dataset into two pieces. The first one, ~ 80% of your dataset is going to be your training data and the left part (~ 20%) gonna be your eval data. This way, you're going to have two folders, **eval/** and **train/** which will contain pictures and their .xml PASCAL VOC representation.\n",
    "\n",
    "```\n",
    "training/\n",
    "-- images/\n",
    "---- eval/\n",
    "------ weapon\n",
    "-------- [files]\n",
    "---- train/\n",
    "------ weapon\n",
    "-------- [files]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Generate training data (TFRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to generate TFRecords that serve as input data to the Tensorflow training model. The following script is going to generate **eval.record** and **train.record** by using pictures and their related .xml PASCAL VOC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecords: /Users/techlead/PycharmProjects/demo/notebook/../training/eval.record\n",
      "Successfully created the TFRecords: /Users/techlead/PycharmProjects/demo/notebook/../training/train.record\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "ROOT_FOLDER = '../training'\n",
    "PATH_TEST = f\"{ROOT_FOLDER}/images/eval/weapon/\"\n",
    "PATH_RECORD_TEST = f\"{ROOT_FOLDER}/eval.record\"\n",
    "PATH_TRAIN = f\"{ROOT_FOLDER}/images/train/weapon/\"\n",
    "PATH_RECORD_TRAIN = f\"{ROOT_FOLDER}/train.record\"\n",
    "\n",
    "IMAGE_FORMAT = b'jpg'\n",
    "\n",
    "\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'weapon':\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def xml_to_tf(path_input, path_output):\n",
    "    writer = tf.io.TFRecordWriter(path_output)\n",
    "\n",
    "    files = os.listdir(path_input)\n",
    "    for file in files:\n",
    "        if file.endswith(\".xml\"):\n",
    "            xmlFile = path_input + file\n",
    "\n",
    "            tree = ET.parse(xmlFile)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            filename = root.find('filename').text\n",
    "            width = int(root.find('size')[0].text)\n",
    "            height = int(root.find('size')[1].text)\n",
    "\n",
    "            x_mins = []\n",
    "            x_maxs = []\n",
    "            y_mins = []\n",
    "            y_maxs = []\n",
    "            classes_text = []\n",
    "            classes = []\n",
    "\n",
    "            for member in root.findall('object'):\n",
    "                name = member[0].text\n",
    "                x_min = int(member[5][0].text)\n",
    "                y_min = int(member[5][1].text)\n",
    "                x_max = int(member[5][2].text)\n",
    "                y_max = int(member[5][3].text)\n",
    "\n",
    "                x_mins.append(x_min / width)\n",
    "                x_maxs.append(x_max / width)\n",
    "                y_mins.append(y_min / height)\n",
    "                y_maxs.append(y_max / height)\n",
    "                classes_text.append(name.encode('utf8'))\n",
    "                classes.append(class_text_to_int(name))\n",
    "\n",
    "            with tf.io.gfile.GFile(os.path.join(path_input, '{}'.format(filename)), 'rb') as fid:\n",
    "                encoded_jpg = fid.read()\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'image/height'            : dataset_util.int64_feature(height),\n",
    "                    'image/width'             : dataset_util.int64_feature(width),\n",
    "                    'image/filename'          : dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "                    'image/source_id'         : dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "                    'image/encoded'           : dataset_util.bytes_feature(encoded_jpg),\n",
    "                    'image/format'            : dataset_util.bytes_feature(IMAGE_FORMAT),\n",
    "                    'image/object/bbox/xmin'  : dataset_util.float_list_feature(x_mins),\n",
    "                    'image/object/bbox/xmax'  : dataset_util.float_list_feature(x_maxs),\n",
    "                    'image/object/bbox/ymin'  : dataset_util.float_list_feature(y_mins),\n",
    "                    'image/object/bbox/ymax'  : dataset_util.float_list_feature(y_maxs),\n",
    "                    'image/object/class/text' : dataset_util.bytes_list_feature(classes_text),\n",
    "                    'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "            }))\n",
    "\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), path_output)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "xml_to_tf(PATH_TEST, PATH_RECORD_TEST)\n",
    "xml_to_tf(PATH_TRAIN, PATH_RECORD_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Create a label map and set the training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label map tells the trainer what each object is by defining a mapping of class names to class ID numbers.\n",
    "\n",
    "Just create a **label_map.pbtxt** and add your classes. In this demo, I'm using one single class: **weapon**.\n",
    "\n",
    "```\n",
    "item {\n",
    "    id: 1\n",
    "    name: 'weapon'\n",
    "}\n",
    "\n",
    "item {\n",
    "    id: 2\n",
    "    name: 'another_class'\n",
    "}\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "Then, you'll have to create a configuration pipeline. I'm using the existing [config for SSDLite MobileNet v2 COCO](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config).\n",
    "\n",
    "Also, grab the fine tune checkpoint for this model: [ssdlite_mobilenet_v2_coco.tar.gz](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz)\n",
    "\n",
    "Replace the number of classes by your number of classes you got (here, 1) and update all PATHS placeholders to your project:\n",
    "\n",
    "> Note: my training data along with the label map and the pipeline config is stored on Google Cloud Storage, because we're going to train our model on **CLOUD AI VISION** service (ex CLOUD ML).\n",
    "> Replace **{bucket_id}** by our own bucket id: **gs://{bucket_id}/training/xxx**\n",
    "\n",
    "```\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 1\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 300\n",
    "        width: 270\n",
    "        resize_method: AREA\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_mobilenet_v2\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 3.99999989895e-05\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.0299999993294\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.999700009823\n",
    "          center: true\n",
    "          scale: true\n",
    "          epsilon: 0.0010000000475\n",
    "          train: true\n",
    "        }\n",
    "      }\n",
    "      use_depthwise: true\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 3.99999989895e-05\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.0299999993294\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.999700009823\n",
    "            center: true\n",
    "            scale: true\n",
    "            epsilon: 0.0010000000475\n",
    "            train: true\n",
    "          }\n",
    "        }\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.800000011921\n",
    "        kernel_size: 3\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        use_depthwise: true\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      ssd_anchor_generator {\n",
    "        num_layers: 6\n",
    "        min_scale: 0.20000000298\n",
    "        max_scale: 0.949999988079\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.333299994469\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 20\n",
    "        max_total_detections: 20\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid {\n",
    "        }\n",
    "      }\n",
    "      hard_example_miner {\n",
    "        num_hard_examples: 3000\n",
    "        iou_threshold: 0.990000009537\n",
    "        loss_type: CLASSIFICATION\n",
    "        max_negatives_per_positive: 3\n",
    "        min_negatives_per_image: 3\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 24\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    ssd_random_crop {\n",
    "    }\n",
    "  }\n",
    "  optimizer {\n",
    "    rms_prop_optimizer {\n",
    "      learning_rate {\n",
    "        exponential_decay_learning_rate {\n",
    "          initial_learning_rate: 0.00400000018999\n",
    "          decay_steps: 800720\n",
    "          decay_factor: 0.949999988079\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.899999976158\n",
    "      decay: 0.899999976158\n",
    "      epsilon: 1.0\n",
    "    }\n",
    "  }\n",
    "  fine_tune_checkpoint: \"gs://{bucket_id}/training/model.ckpt\"\n",
    "  num_steps: 200000\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"gs://{bucket_id}/training/label_map.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"gs://{bucket_id}/training/train.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  num_examples: 8000\n",
    "  max_evals: 10\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"gs://{bucket_id}/training/label_map.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"gs://{bucket_id}/training/eval.record\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Check'in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your GCS Bucket should contains a **training/** folder holding theses files:\n",
    "\n",
    "- eval.record\n",
    "- train.record\n",
    "- label_map.pbtxt\n",
    "- ssdlite_mobilenet_v2_coco.config\n",
    "- model.ckpt.* (x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Set the training cluster configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a **cloud.yml** file and put that config in it. You can obviously changes settings to fit your needs.\n",
    "\n",
    "```\n",
    "trainingInput:\n",
    "  runtimeVersion: \"1.12\"\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: standard_gpu\n",
    "  workerCount: 5\n",
    "  workerType: standard_gpu\n",
    "  parameterServerCount: 3\n",
    "  parameterServerType: standard\n",
    "```\n",
    "\n",
    "More info [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Run the training on AI VISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-48bb6b4f586b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-48bb6b4f586b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    gcloud ai-platform jobs submit training object_detection_`date +%m_%d_%Y_%H_%M_%S` \\\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "gcloud ai-platform jobs submit training object_detection_`date +%m_%d_%Y_%H_%M_%S` \\\n",
    "    --runtime-version 1.12 \\\n",
    "    --job-dir gs://{bucket_id}/train \\\n",
    "    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\\n",
    "    --module-name object_detection.model_main \\\n",
    "    --region us-east1 \\\n",
    "    --config cloud.yml \\\n",
    "    -- \\\n",
    "    --model_dir gs://{bucket_id}/train \\\n",
    "    --pipeline_config_path gs://{bucket_id}/training/ssdlite_mobilenet_v2_coco.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to take a while until you achieve a good loss depending on the volume of your dataset, the number of classes etc.\n",
    "\n",
    "You can monitor the job by running `tensorboard --logdir=gs://{bucket_id}/train --host localhost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Export Inference Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the frozen inference graph (.pb), you'll have to download the most recent model.ckpt.* files from your GCS bucket.\n",
    "\n",
    "Next, go to your Tensorflow **model/research/object_detection/** folder and run this command `python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt --output_directory inference_graph\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Use your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-03f970e869b7>:10: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "PATH/TO/THE/frozen_inference_graph.pb; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-03f970e869b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-03f970e869b7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, precision)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFROZEN_INFERENCE_GRAPH_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/demo/virtual_env/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/demo/virtual_env/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     83\u001b[0m       self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 84\u001b[0;31m           compat.as_bytes(self.__name), 1024 * 512)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: PATH/TO/THE/frozen_inference_graph.pb; No such file or directory"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "\n",
    "FROZEN_INFERENCE_GRAPH_PATH = 'PATH/TO/THE/frozen_inference_graph.pb'\n",
    "\n",
    "\n",
    "class Detector(object):\n",
    "    def __init__(self, precision=0.3):\n",
    "        self.precision = precision\n",
    "        with tf.gfile.FastGFile(FROZEN_INFERENCE_GRAPH_PATH, 'rb') as f:\n",
    "            self.graph_def = tf.compat.v1.GraphDef()\n",
    "            self.graph_def.ParseFromString(f.read())\n",
    "\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "        self.sess.graph.as_default()\n",
    "        tf.import_graph_def(self.graph_def, name='')\n",
    "\n",
    "    def detect(self, img):\n",
    "        rows = img.shape[0]\n",
    "        cols = img.shape[1]\n",
    "        inp = cv.resize(img, (300, 270))\n",
    "        inp = inp[:, :, [2, 1, 0]]\n",
    "\n",
    "        # Run the model\n",
    "        out = self.sess.run([\n",
    "            self.sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "            self.sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "            self.sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "            self.sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "        ], feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "\n",
    "        # Visualize detected bounding boxes.\n",
    "        num_detections = int(out[0][0])\n",
    "        for i in range(num_detections):\n",
    "            _ = int(out[3][0][i])  # Index\n",
    "            score = float(out[1][0][i])\n",
    "            bbox = [float(v) for v in out[2][0][i]]\n",
    "\n",
    "            if score > self.precision:\n",
    "                x = bbox[1] * cols\n",
    "                y = bbox[0] * rows\n",
    "                right = bbox[3] * cols\n",
    "                bottom = bbox[2] * rows\n",
    "                cv.rectangle(img,\n",
    "                             (int(x), int(y)),\n",
    "                             (int(right), int(bottom)),\n",
    "                             (125, 255, 51),\n",
    "                             thickness=2)\n",
    "\n",
    "        cv.imshow('Detections', img)\n",
    "        cv.waitKey(0)\n",
    "        \n",
    "detector = Detector()\n",
    "detector.detect(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
